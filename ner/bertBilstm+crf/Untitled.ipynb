{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnrecognizedFlagError",
     "evalue": "Unknown command line flag 'f'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnrecognizedFlagError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a4897f3a65b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[0mFLAGS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m5.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"gradient clip should't be too much\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;32massert\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dropout rate between 0 and 1\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"learning rate must larger than zero\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\flags.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;31m# a flag.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_parsed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m       \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\absl\\flags\\_flagvalues.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, argv, known_only)\u001b[0m\n\u001b[0;32m    631\u001b[0m       \u001b[0msuggestions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_helpers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_flag_suggestions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m       raise _exceptions.UnrecognizedFlagError(\n\u001b[1;32m--> 633\u001b[1;33m           name, value, suggestions=suggestions)\n\u001b[0m\u001b[0;32m    634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmark_as_parsed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnrecognizedFlagError\u001b[0m: Unknown command line flag 'f'"
     ]
    }
   ],
   "source": [
    "# encoding=utf8\n",
    "import os\n",
    "import codecs\n",
    "import pickle\n",
    "import itertools\n",
    "from collections import OrderedDict\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from model import Model\n",
    "from loader import load_sentences, update_tag_scheme\n",
    "from loader import char_mapping, tag_mapping\n",
    "from loader import augment_with_pretrained, prepare_dataset\n",
    "from utils import get_logger, make_path, clean, create_model, save_model\n",
    "from utils import print_config, save_config, load_config, test_ner\n",
    "from data_utils import create_input, BatchManager\n",
    "\n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_boolean(\"clean\",           False,      \"clean train folder\")\n",
    "flags.DEFINE_boolean(\"train\",           False,      \"Wither train the model\")\n",
    "# configurations for the model\n",
    "flags.DEFINE_integer(\"batch_size\",      5,         \"batch size\")\n",
    "flags.DEFINE_integer(\"seg_dim\",         20,         \"Embedding size for segmentation, 0 if not used\")\n",
    "flags.DEFINE_integer(\"char_dim\",        100,        \"Embedding size for characters\")\n",
    "flags.DEFINE_integer(\"lstm_dim\",        100,        \"Num of hidden units in LSTM\")\n",
    "# 改：flags.DEFINE_integer(\"lstm_dim\",      200,        \"Num of hidden units in LSTM\")\n",
    "flags.DEFINE_string(\"tag_schema\",       \"iobes\",      \"tagging schema iobes or iob\")\n",
    "\n",
    "# configurations for training\n",
    "flags.DEFINE_float(\"clip\",              5,          \"Gradient clip\")\n",
    "flags.DEFINE_float(\"dropout\",           0.5,        \"Dropout rate\")\n",
    "flags.DEFINE_float(\"lr\",                0.001,      \"Initial learning rate\")\n",
    "flags.DEFINE_string(\"optimizer\",        \"adam\",     \"Optimizer for training\")\n",
    "flags.DEFINE_boolean(\"zeros\",           False,      \"Wither replace digits with zero\")\n",
    "flags.DEFINE_boolean(\"lower\",           True,       \"Wither lower case\")\n",
    "\n",
    "flags.DEFINE_integer(\"max_seq_len\",     128,        \"max sequence length for bert\")\n",
    "flags.DEFINE_integer(\"max_epoch\",       100,        \"maximum training epochs\")\n",
    "flags.DEFINE_integer(\"steps_check\",     1,          \"steps per checkpoint\")\n",
    "flags.DEFINE_string(\"ckpt_path\",        \"ckpt\",      \"Path to save model\")\n",
    "flags.DEFINE_string(\"bilstm_ckpt_path\", \"bilstm_ckpt\",  \"Path to restore bilstm_model\")\n",
    "flags.DEFINE_string(\"summary_path\",     \"summary\",      \"Path to store summaries\")\n",
    "flags.DEFINE_string(\"log_file\",         \"train.log\",    \"File for log\")\n",
    "flags.DEFINE_string(\"map_file\",         \"maps.pkl\",     \"file for maps\")\n",
    "flags.DEFINE_string(\"vocab_file\",       \"vocab.json\",   \"File for vocab\")\n",
    "flags.DEFINE_string(\"config_file\",      \"config_file\",  \"File for config\")\n",
    "flags.DEFINE_string(\"script\",           \"conlleval\",    \"evaluation script\")\n",
    "flags.DEFINE_string(\"result_path\",      \"result\",       \"Path for results\")\n",
    "import os\n",
    "flags.DEFINE_string(\"train_file\",       os.path.join(\"data\", \"example.train\"),  \"Path for train data\")\n",
    "flags.DEFINE_string(\"dev_file\",         os.path.join(\"data\", \"example.dev\"),    \"Path for dev data\")\n",
    "flags.DEFINE_string(\"test_file\",        os.path.join(\"data\", \"example.test\"),   \"Path for test data\")\n",
    "\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "assert FLAGS.clip < 5.1, \"gradient clip should't be too much\"\n",
    "assert 0 <= FLAGS.dropout < 1, \"dropout rate between 0 and 1\"\n",
    "assert FLAGS.lr > 0, \"learning rate must larger than zero\"\n",
    "assert FLAGS.optimizer in [\"adam\", \"sgd\", \"adagrad\"]\n",
    "\n",
    "\n",
    "# config for the model\n",
    "def config_model(tag_to_id):\n",
    "    config = OrderedDict()\n",
    "    config[\"num_tags\"] = len(tag_to_id)\n",
    "    config[\"lstm_dim\"] = FLAGS.lstm_dim\n",
    "    config[\"batch_size\"] = FLAGS.batch_size\n",
    "    config['max_seq_len'] = FLAGS.max_seq_len\n",
    "\n",
    "    config[\"clip\"] = FLAGS.clip\n",
    "    config[\"dropout_keep\"] = 1.0 - FLAGS.dropout\n",
    "    config[\"optimizer\"] = FLAGS.optimizer\n",
    "    config[\"lr\"] = FLAGS.lr\n",
    "    config[\"tag_schema\"] = FLAGS.tag_schema\n",
    "    config[\"zeros\"] = FLAGS.zeros\n",
    "    config[\"lower\"] = FLAGS.lower\n",
    "    return config\n",
    "\n",
    "def evaluate(sess, model, name, data, id_to_tag, logger):\n",
    "    logger.info(\"evaluate:{}\".format(name))\n",
    "    ner_results = model.evaluate(sess, data, id_to_tag)\n",
    "    eval_lines = test_ner(ner_results, FLAGS.result_path)\n",
    "    for line in eval_lines:\n",
    "        logger.info(line)\n",
    "    f1 = float(eval_lines[1].strip().split()[-1])\n",
    "\n",
    "    if name == \"dev\":\n",
    "        best_test_f1 = model.best_dev_f1.eval()\n",
    "        if f1 > best_test_f1:\n",
    "            tf.assign(model.best_dev_f1, f1).eval()\n",
    "            logger.info(\"new best dev f1 score:{:>.3f}\".format(f1))\n",
    "        return f1 > best_test_f1\n",
    "    elif name == \"test\":\n",
    "        best_test_f1 = model.best_test_f1.eval()\n",
    "        if f1 > best_test_f1:\n",
    "            tf.assign(model.best_test_f1, f1).eval()\n",
    "            logger.info(\"new best test f1 score:{:>.3f}\".format(f1))\n",
    "        return f1 > best_test_f1\n",
    "\n",
    "def train():\n",
    "    # load data sets\n",
    "    train_sentences = load_sentences(FLAGS.train_file, FLAGS.lower, FLAGS.zeros)\n",
    "    dev_sentences = load_sentences(FLAGS.dev_file, FLAGS.lower, FLAGS.zeros)\n",
    "    # test_sentences = load_sentences(FLAGS.test_file, FLAGS.lower, FLAGS.zeros)\n",
    "\n",
    "    # Use selected tagging scheme (IOB / IOBES)\n",
    "    update_tag_scheme(train_sentences, FLAGS.tag_schema)\n",
    "    # update_tag_scheme(test_sentences, FLAGS.tag_schema)\n",
    "\n",
    "    # create maps if not exist\n",
    "    if not os.path.isfile(FLAGS.map_file):\n",
    "        # Create a dictionary and a mapping for tags\n",
    "        _t, tag_to_id, id_to_tag = tag_mapping(train_sentences)\n",
    "        with open(FLAGS.map_file, \"wb\") as f:\n",
    "            pickle.dump([tag_to_id, id_to_tag], f)\n",
    "    else:\n",
    "        with open(FLAGS.map_file, \"rb\") as f:\n",
    "            tag_to_id, id_to_tag = pickle.load(f)\n",
    "\n",
    "    # prepare data, get a collection of list containing index\n",
    "    train_data = prepare_dataset(\n",
    "        train_sentences, FLAGS.max_seq_len, tag_to_id, FLAGS.lower\n",
    "    )\n",
    "    dev_data = prepare_dataset(\n",
    "        dev_sentences, FLAGS.max_seq_len, tag_to_id, FLAGS.lower\n",
    "    )\n",
    "    # test_data = prepare_dataset(\n",
    "    #     test_sentences, FLAGS.max_seq_len, tag_to_id, FLAGS.lower\n",
    "    # )\n",
    "    print(\"%i / %i / %i sentences in train / dev / test.\" % (\n",
    "        len(train_data), 0, len(dev_data)))\n",
    "\n",
    "    train_manager = BatchManager(train_data, FLAGS.batch_size)\n",
    "    dev_manager = BatchManager(dev_data, FLAGS.batch_size)\n",
    "    # test_manager = BatchManager(test_data, FLAGS.batch_size)\n",
    "    # make path for store log and model if not exist\n",
    "    make_path(FLAGS)\n",
    "    if os.path.isfile(FLAGS.config_file):\n",
    "        config = load_config(FLAGS.config_file)\n",
    "    else:\n",
    "        config = config_model(tag_to_id)\n",
    "        save_config(config, FLAGS.config_file)\n",
    "    make_path(FLAGS)\n",
    "\n",
    "    log_path = os.path.join(\"log\", FLAGS.log_file)\n",
    "    logger = get_logger(log_path)\n",
    "    print_config(config, logger)\n",
    "\n",
    "    # limit GPU memory\n",
    "    tf_config = tf.ConfigProto()\n",
    "    tf_config.gpu_options.allow_growth = True\n",
    "    steps_per_epoch = train_manager.len_data\n",
    "\n",
    "    with tf.Session(config=tf_config) as sess:\n",
    "        model = create_model(sess, Model, FLAGS.ckpt_path, FLAGS.bilstm_ckpt_path, config, logger)\n",
    "\n",
    "        logger.info(\"start training\")\n",
    "        loss = []\n",
    "        for i in range(100):\n",
    "            for batch in train_manager.iter_batch(shuffle=True):\n",
    "                step, batch_loss = model.run_step(sess, True, batch)\n",
    "\n",
    "                loss.append(batch_loss)\n",
    "                if step % FLAGS.steps_check == 0:\n",
    "                    iteration = step // steps_per_epoch + 1\n",
    "                    logger.info(\"iteration:{} step:{}/{}, \"\n",
    "                                \"NER loss:{:>9.6f}\".format(\n",
    "                        iteration, step%steps_per_epoch, steps_per_epoch, np.mean(loss)))\n",
    "                    loss = []\n",
    "\n",
    "            best = evaluate(sess, model, \"dev\", dev_manager, id_to_tag, logger)\n",
    "            if best:\n",
    "                save_model(sess, model, FLAGS.ckpt_path, logger, global_steps=step)\n",
    "            # evaluate(sess, model, \"test\", test_manager, id_to_tag, logger)\n",
    "\n",
    "def main(_):\n",
    "    FLAGS.train = True\n",
    "    FLAGS.clean = False\n",
    "    if FLAGS.clean:\n",
    "        clean(FLAGS)\n",
    "    train()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "    tf.app.run(main)\n",
    "\n",
    "    # loss:287.421570   loss:32.542763"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
