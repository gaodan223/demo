{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\leo\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.225 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# utf-8\n",
    "import codecs\n",
    "import pickle\n",
    "import itertools\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from model import Model\n",
    "from loader import load_sentences, update_tag_scheme\n",
    "from loader import char_mapping, tag_mapping\n",
    "from loader import augment_with_pretrained, prepare_dataset\n",
    "from utils import get_logger, make_path, clean, create_model, save_model\n",
    "from utils import print_config, save_config, load_config, test_ner\n",
    "from data_utils import load_word2vec, create_input, input_from_line, BatchManager\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_boolean(\"clean\",       False,      \"clean train folder\")\n",
    "flags.DEFINE_boolean(\"train\",       True,      \"Wither train the model\")\n",
    "# configurations for the model\n",
    "flags.DEFINE_integer(\"seg_dim\",     20,         \"Embedding size for segmentation, 0 if not used\")\n",
    "flags.DEFINE_integer(\"char_dim\",    100,        \"Embedding size for characters\")\n",
    "flags.DEFINE_integer(\"lstm_dim\",    100,        \"Num of hidden units in LSTM\")\n",
    "flags.DEFINE_string(\"tag_schema\",   \"iobes\",    \"tagging schema iobes or iob\")\n",
    "# configurations for training\n",
    "flags.DEFINE_float(\"clip\",          5,          \"Gradient clip\")\n",
    "flags.DEFINE_float(\"dropout\",       0.5,        \"Dropout rate\")\n",
    "flags.DEFINE_float(\"batch_size\",    20,         \"batch size\")\n",
    "flags.DEFINE_float(\"lr\",            0.001,      \"Initial learning rate\")\n",
    "flags.DEFINE_string(\"optimizer\",    \"adam\",     \"Optimizer for training\")\n",
    "flags.DEFINE_boolean(\"pre_emb\",     True,       \"Wither use pre-trained embedding\")\n",
    "flags.DEFINE_boolean(\"zeros\",       False,      \"Wither replace digits with zero\")\n",
    "flags.DEFINE_boolean(\"lower\",       True,       \"Wither lower case\")\n",
    "flags.DEFINE_integer(\"max_epoch\",   100,        \"maximum training epochs\")\n",
    "flags.DEFINE_integer(\"steps_check\", 100,        \"steps per checkpoint\")\n",
    "flags.DEFINE_string(\"ckpt_path\",    \"ckpt\",      \"Path to save model\")\n",
    "flags.DEFINE_string(\"summary_path\", \"summary\",      \"Path to store summaries\")\n",
    "flags.DEFINE_string(\"log_file\",     \"train.log\",    \"File for log\")\n",
    "flags.DEFINE_string(\"map_file\",     \"maps.pkl\",     \"file for maps\")\n",
    "flags.DEFINE_string(\"vocab_file\",   \"vocab.json\",   \"File for vocab\")\n",
    "flags.DEFINE_string(\"config_file\",  \"config_file\",  \"File for config\")\n",
    "flags.DEFINE_string(\"script\",       \"conlleval\",    \"evaluation script\")\n",
    "flags.DEFINE_string(\"result_path\",  \"result\",       \"Path for results\")\n",
    "flags.DEFINE_string(\"emb_file\",     \"wiki_100.utf8\", \"Path for pre_trained embedding\")\n",
    "flags.DEFINE_string(\"train_file\",   os.path.join(\"data\", \"example.train\"),  \"Path for train data\")\n",
    "flags.DEFINE_string(\"dev_file\",     os.path.join(\"data\", \"example.dev\"),    \"Path for dev data\")\n",
    "flags.DEFINE_string(\"test_file\",    os.path.join(\"data\", \"example.test\"),   \"Path for test data\")\n",
    "\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "assert FLAGS.clip < 5.1, \"gradient clip should't be too much\"\n",
    "assert 0 <= FLAGS.dropout < 1, \"dropout rate between 0 and 1\"\n",
    "assert FLAGS.lr > 0, \"learning rate must larger than zero\"\n",
    "assert FLAGS.optimizer in [\"adam\", \"sgd\", \"adagrad\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5380 / 0 / 1599 sentences in train / dev / test.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-13 07:47:01,709 - log\\train.log - INFO - num_chars      :\t4178\n",
      "2020-01-13 07:47:01,719 - log\\train.log - INFO - char_dim       :\t100\n",
      "2020-01-13 07:47:01,722 - log\\train.log - INFO - num_tags       :\t13\n",
      "2020-01-13 07:47:01,723 - log\\train.log - INFO - seg_dim        :\t20\n",
      "2020-01-13 07:47:01,724 - log\\train.log - INFO - lstm_dim       :\t100\n",
      "2020-01-13 07:47:01,726 - log\\train.log - INFO - batch_size     :\t20.0\n",
      "2020-01-13 07:47:01,729 - log\\train.log - INFO - emb_file       :\twiki_100.utf8\n",
      "2020-01-13 07:47:01,731 - log\\train.log - INFO - clip           :\t5.0\n",
      "2020-01-13 07:47:01,734 - log\\train.log - INFO - dropout_keep   :\t0.5\n",
      "2020-01-13 07:47:01,736 - log\\train.log - INFO - optimizer      :\tadam\n",
      "2020-01-13 07:47:01,738 - log\\train.log - INFO - lr             :\t0.001\n",
      "2020-01-13 07:47:01,741 - log\\train.log - INFO - tag_schema     :\tiobes\n",
      "2020-01-13 07:47:01,743 - log\\train.log - INFO - pre_emb        :\tTrue\n",
      "2020-01-13 07:47:01,745 - log\\train.log - INFO - zeros          :\tFalse\n",
      "2020-01-13 07:47:01,746 - log\\train.log - INFO - lower          :\tTrue\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\leo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:417: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n",
      "WARNING:tensorflow:From C:\\Users\\leo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:432: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-13 07:47:04,605 - log\\train.log - INFO - Reading model parameters from ckpt\\ner.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ckpt\\ner.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-13 07:47:04,730 - log\\train.log - INFO - start training\n",
      "2020-01-13 07:47:49,837 - log\\train.log - INFO - iteration:129 step:68/269, NER loss: 3.347261\n",
      "2020-01-13 07:48:37,047 - log\\train.log - INFO - iteration:129 step:168/269, NER loss: 2.652917\n",
      "2020-01-13 07:49:35,939 - log\\train.log - INFO - iteration:129 step:268/269, NER loss: 3.136491\n",
      "2020-01-13 07:49:36,202 - log\\train.log - INFO - evaluate:dev\n",
      "2020-01-13 07:49:52,662 - log\\train.log - INFO - processed 261378 tokens with 19981 phrases; found: 20204 phrases; correct: 18054.\n",
      "\n",
      "2020-01-13 07:49:52,665 - log\\train.log - INFO - accuracy:  96.65%; precision:  89.36%; recall:  90.36%; FB1:  89.85\n",
      "\n",
      "2020-01-13 07:49:52,666 - log\\train.log - INFO -              BODY: precision:  93.70%; recall:  93.77%; FB1:  93.73  8477\n",
      "\n",
      "2020-01-13 07:49:52,668 - log\\train.log - INFO -           DISEASE: precision:  84.96%; recall:  88.08%; FB1:  86.49  7494\n",
      "\n",
      "2020-01-13 07:49:52,670 - log\\train.log - INFO -           SYMPTOM: precision:  88.45%; recall:  87.46%; FB1:  87.95  4233\n",
      "\n",
      "2020-01-13 07:50:52,544 - log\\train.log - INFO - iteration:130 step:99/269, NER loss: 3.016650\n",
      "2020-01-13 07:51:46,105 - log\\train.log - INFO - iteration:130 step:199/269, NER loss: 3.059619\n",
      "2020-01-13 07:52:19,523 - log\\train.log - INFO - evaluate:dev\n",
      "2020-01-13 07:52:47,693 - log\\train.log - INFO - processed 261378 tokens with 19981 phrases; found: 20075 phrases; correct: 18015.\n",
      "\n",
      "2020-01-13 07:52:47,696 - log\\train.log - INFO - accuracy:  96.80%; precision:  89.74%; recall:  90.16%; FB1:  89.95\n",
      "\n",
      "2020-01-13 07:52:47,697 - log\\train.log - INFO -              BODY: precision:  93.46%; recall:  94.27%; FB1:  93.86  8545\n",
      "\n",
      "2020-01-13 07:52:47,699 - log\\train.log - INFO -           DISEASE: precision:  85.26%; recall:  87.48%; FB1:  86.36  7417\n",
      "\n",
      "2020-01-13 07:52:47,701 - log\\train.log - INFO -           SYMPTOM: precision:  90.08%; recall:  86.55%; FB1:  88.28  4113\n",
      "\n",
      "2020-01-13 07:53:00,737 - log\\train.log - INFO - iteration:131 step:30/269, NER loss: 2.543550\n",
      "2020-01-13 07:54:17,835 - log\\train.log - INFO - iteration:131 step:130/269, NER loss: 3.491481\n",
      "2020-01-13 07:55:08,010 - log\\train.log - INFO - iteration:131 step:230/269, NER loss: 2.839237\n",
      "2020-01-13 07:55:19,430 - log\\train.log - INFO - evaluate:dev\n",
      "2020-01-13 07:55:36,949 - log\\train.log - INFO - processed 261378 tokens with 19981 phrases; found: 20185 phrases; correct: 18001.\n",
      "\n",
      "2020-01-13 07:55:36,953 - log\\train.log - INFO - accuracy:  96.57%; precision:  89.18%; recall:  90.09%; FB1:  89.63\n",
      "\n",
      "2020-01-13 07:55:36,954 - log\\train.log - INFO -              BODY: precision:  94.26%; recall:  93.24%; FB1:  93.74  8379\n",
      "\n",
      "2020-01-13 07:55:36,955 - log\\train.log - INFO -           DISEASE: precision:  83.69%; recall:  88.59%; FB1:  86.07  7652\n",
      "\n",
      "2020-01-13 07:55:36,957 - log\\train.log - INFO -           SYMPTOM: precision:  89.05%; recall:  86.41%; FB1:  87.71  4154\n",
      "\n",
      "2020-01-13 07:56:02,222 - log\\train.log - INFO - iteration:132 step:61/269, NER loss: 2.179865\n",
      "2020-01-13 07:56:44,419 - log\\train.log - INFO - iteration:132 step:161/269, NER loss: 2.321828\n",
      "2020-01-13 07:58:14,800 - log\\train.log - INFO - iteration:132 step:261/269, NER loss: 3.655858\n",
      "2020-01-13 07:58:19,593 - log\\train.log - INFO - evaluate:dev\n",
      "2020-01-13 07:58:43,194 - log\\train.log - INFO - processed 261378 tokens with 19981 phrases; found: 20184 phrases; correct: 18057.\n",
      "\n",
      "2020-01-13 07:58:43,198 - log\\train.log - INFO - accuracy:  96.76%; precision:  89.46%; recall:  90.37%; FB1:  89.91\n",
      "\n",
      "2020-01-13 07:58:43,200 - log\\train.log - INFO -              BODY: precision:  93.30%; recall:  94.13%; FB1:  93.71  8547\n",
      "\n",
      "2020-01-13 07:58:43,206 - log\\train.log - INFO -           DISEASE: precision:  85.78%; recall:  87.65%; FB1:  86.71  7386\n",
      "\n",
      "2020-01-13 07:58:43,210 - log\\train.log - INFO -           SYMPTOM: precision:  88.14%; recall:  87.53%; FB1:  87.83  4251\n",
      "\n",
      "2020-01-13 07:59:29,506 - log\\train.log - INFO - iteration:133 step:92/269, NER loss: 2.405964\n",
      "2020-01-13 08:00:47,959 - log\\train.log - INFO - iteration:133 step:192/269, NER loss: 3.633904\n",
      "2020-01-13 08:01:24,294 - log\\train.log - INFO - evaluate:dev\n",
      "2020-01-13 08:01:42,322 - log\\train.log - INFO - processed 261378 tokens with 19981 phrases; found: 20191 phrases; correct: 18036.\n",
      "\n",
      "2020-01-13 08:01:42,326 - log\\train.log - INFO - accuracy:  96.66%; precision:  89.33%; recall:  90.27%; FB1:  89.79\n",
      "\n",
      "2020-01-13 08:01:42,327 - log\\train.log - INFO -              BODY: precision:  94.19%; recall:  93.66%; FB1:  93.93  8423\n",
      "\n",
      "2020-01-13 08:01:42,328 - log\\train.log - INFO -           DISEASE: precision:  83.99%; recall:  88.12%; FB1:  86.01  7584\n",
      "\n",
      "2020-01-13 08:01:42,330 - log\\train.log - INFO -           SYMPTOM: precision:  89.20%; recall:  87.18%; FB1:  88.17  4184\n",
      "\n",
      "2020-01-13 08:02:13,010 - log\\train.log - INFO - iteration:134 step:23/269, NER loss: 2.837127\n",
      "2020-01-13 08:03:09,608 - log\\train.log - INFO - iteration:134 step:123/269, NER loss: 2.903136\n",
      "2020-01-13 08:04:02,861 - log\\train.log - INFO - iteration:134 step:223/269, NER loss: 2.881681\n",
      "2020-01-13 08:04:24,349 - log\\train.log - INFO - evaluate:dev\n",
      "2020-01-13 08:04:46,982 - log\\train.log - INFO - processed 261378 tokens with 19981 phrases; found: 20159 phrases; correct: 18014.\n",
      "\n",
      "2020-01-13 08:04:46,985 - log\\train.log - INFO - accuracy:  96.69%; precision:  89.36%; recall:  90.16%; FB1:  89.76\n",
      "\n",
      "2020-01-13 08:04:46,986 - log\\train.log - INFO -              BODY: precision:  93.11%; recall:  93.77%; FB1:  93.44  8531\n",
      "\n",
      "2020-01-13 08:04:46,987 - log\\train.log - INFO -           DISEASE: precision:  84.90%; recall:  87.83%; FB1:  86.34  7478\n",
      "\n",
      "2020-01-13 08:04:46,989 - log\\train.log - INFO -           SYMPTOM: precision:  89.69%; recall:  86.94%; FB1:  88.29  4150\n",
      "\n",
      "2020-01-13 08:05:32,783 - log\\train.log - INFO - iteration:135 step:54/269, NER loss: 2.603354\n",
      "2020-01-13 08:06:28,702 - log\\train.log - INFO - iteration:135 step:154/269, NER loss: 2.902122\n",
      "2020-01-13 08:07:19,349 - log\\train.log - INFO - iteration:135 step:254/269, NER loss: 2.881679\n",
      "2020-01-13 08:07:25,338 - log\\train.log - INFO - evaluate:dev\n",
      "2020-01-13 08:07:43,952 - log\\train.log - INFO - processed 261378 tokens with 19981 phrases; found: 20307 phrases; correct: 18089.\n",
      "\n",
      "2020-01-13 08:07:43,956 - log\\train.log - INFO - accuracy:  96.66%; precision:  89.08%; recall:  90.53%; FB1:  89.80\n",
      "\n",
      "2020-01-13 08:07:43,956 - log\\train.log - INFO -              BODY: precision:  93.24%; recall:  94.06%; FB1:  93.65  8546\n",
      "\n",
      "2020-01-13 08:07:43,958 - log\\train.log - INFO -           DISEASE: precision:  84.33%; recall:  88.14%; FB1:  86.20  7556\n",
      "\n",
      "2020-01-13 08:07:43,959 - log\\train.log - INFO -           SYMPTOM: precision:  89.16%; recall:  87.57%; FB1:  88.36  4205\n",
      "\n",
      "2020-01-13 08:08:19,210 - log\\train.log - INFO - iteration:136 step:85/269, NER loss: 2.068175\n"
     ]
    }
   ],
   "source": [
    "# config for the model\n",
    "def config_model(char_to_id, tag_to_id):\n",
    "    config = OrderedDict()\n",
    "    config[\"num_chars\"] = len(char_to_id)\n",
    "    config[\"char_dim\"] = FLAGS.char_dim\n",
    "    config[\"num_tags\"] = len(tag_to_id)\n",
    "    config[\"seg_dim\"] = FLAGS.seg_dim\n",
    "    config[\"lstm_dim\"] = FLAGS.lstm_dim\n",
    "    config[\"batch_size\"] = FLAGS.batch_size\n",
    "\n",
    "    config[\"emb_file\"] = FLAGS.emb_file\n",
    "    config[\"clip\"] = FLAGS.clip\n",
    "    config[\"dropout_keep\"] = 1.0 - FLAGS.dropout\n",
    "    config[\"optimizer\"] = FLAGS.optimizer\n",
    "    config[\"lr\"] = FLAGS.lr\n",
    "    config[\"tag_schema\"] = FLAGS.tag_schema\n",
    "    config[\"pre_emb\"] = FLAGS.pre_emb\n",
    "    config[\"zeros\"] = FLAGS.zeros\n",
    "    config[\"lower\"] = FLAGS.lower\n",
    "    return config\n",
    "\n",
    "\n",
    "def evaluate(sess, model, name, data, id_to_tag, logger):\n",
    "    logger.info(\"evaluate:{}\".format(name))\n",
    "    ner_results = model.evaluate(sess, data, id_to_tag)\n",
    "    eval_lines = test_ner(ner_results, FLAGS.result_path)\n",
    "    for line in eval_lines:\n",
    "        logger.info(line)\n",
    "    f1 = float(eval_lines[1].strip().split()[-1])\n",
    "\n",
    "    if name == \"dev\":\n",
    "        best_test_f1 = model.best_dev_f1.eval()\n",
    "        if f1 > best_test_f1:\n",
    "            tf.assign(model.best_dev_f1, f1).eval()\n",
    "            logger.info(\"new best dev f1 score:{:>.3f}\".format(f1))\n",
    "        return f1 > best_test_f1\n",
    "    elif name == \"test\":\n",
    "        best_test_f1 = model.best_test_f1.eval()\n",
    "        if f1 > best_test_f1:\n",
    "            tf.assign(model.best_test_f1, f1).eval()\n",
    "            logger.info(\"new best test f1 score:{:>.3f}\".format(f1))\n",
    "        return f1 > best_test_f1\n",
    "\n",
    "\n",
    "def train():\n",
    "    # load data sets\n",
    "    train_sentences = load_sentences(FLAGS.train_file, FLAGS.lower, FLAGS.zeros)\n",
    "    dev_sentences = load_sentences(FLAGS.dev_file, FLAGS.lower, FLAGS.zeros)\n",
    "    test_sentences = load_sentences(FLAGS.test_file, FLAGS.lower, FLAGS.zeros)\n",
    "\n",
    "    # Use selected tagging scheme (IOB / IOBES)\n",
    "    update_tag_scheme(train_sentences, FLAGS.tag_schema)\n",
    "    update_tag_scheme(test_sentences, FLAGS.tag_schema)\n",
    "\n",
    "    # create maps if not exist\n",
    "    if not os.path.isfile(FLAGS.map_file):\n",
    "        # create dictionary for word\n",
    "        if FLAGS.pre_emb:\n",
    "            dico_chars_train = char_mapping(train_sentences, FLAGS.lower)[0]\n",
    "            dico_chars, char_to_id, id_to_char = augment_with_pretrained(\n",
    "                dico_chars_train.copy(),\n",
    "                FLAGS.emb_file,\n",
    "                list(itertools.chain.from_iterable(\n",
    "                    [[w[0] for w in s] for s in test_sentences])\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            _c, char_to_id, id_to_char = char_mapping(train_sentences, FLAGS.lower)\n",
    "\n",
    "        # Create a dictionary and a mapping for tags\n",
    "        _t, tag_to_id, id_to_tag = tag_mapping(train_sentences)\n",
    "        with open(FLAGS.map_file, \"wb\") as f:\n",
    "            pickle.dump([char_to_id, id_to_char, tag_to_id, id_to_tag], f)\n",
    "    else:\n",
    "        with open(FLAGS.map_file, \"rb\") as f:\n",
    "            char_to_id, id_to_char, tag_to_id, id_to_tag = pickle.load(f)\n",
    "\n",
    "    # prepare data, get a collection of list containing index\n",
    "    train_data = prepare_dataset(\n",
    "        train_sentences, char_to_id, tag_to_id, FLAGS.lower\n",
    "    )\n",
    "    dev_data = prepare_dataset(\n",
    "        dev_sentences, char_to_id, tag_to_id, FLAGS.lower\n",
    "    )\n",
    "    # test_data = prepare_dataset(\n",
    "    #     test_sentences, char_to_id, tag_to_id, FLAGS.lower\n",
    "    # )\n",
    "    # print(\"%i / %i / %i sentences in train / dev / test.\" % (\n",
    "    #     len(train_data), 0, len(test_data)))\n",
    "    print(\"%i / %i / %i sentences in train / dev / test.\" % (\n",
    "        len(train_data), 0, len(dev_data)))\n",
    "\n",
    "    train_manager = BatchManager(train_data, FLAGS.batch_size)\n",
    "    dev_manager = BatchManager(dev_data, 100)\n",
    "    # test_manager = BatchManager(test_data, 100)\n",
    "    # make path for store log and model if not exist\n",
    "    make_path(FLAGS)\n",
    "    if os.path.isfile(FLAGS.config_file):\n",
    "        config = load_config(FLAGS.config_file)\n",
    "    else:\n",
    "        config = config_model(char_to_id, tag_to_id)\n",
    "        save_config(config, FLAGS.config_file)\n",
    "    make_path(FLAGS)\n",
    "\n",
    "    log_path = os.path.join(\"log\", FLAGS.log_file)\n",
    "    logger = get_logger(log_path)\n",
    "    print_config(config, logger)\n",
    "\n",
    "    # limit GPU memory\n",
    "    tf_config = tf.ConfigProto()\n",
    "    tf_config.gpu_options.allow_growth = True\n",
    "    steps_per_epoch = train_manager.len_data\n",
    "    with tf.Session(config=tf_config) as sess:\n",
    "        model = create_model(sess, Model, FLAGS.ckpt_path, load_word2vec, config, id_to_char, logger)\n",
    "        logger.info(\"start training\")\n",
    "        loss = []\n",
    "        for i in range(100):\n",
    "            for batch in train_manager.iter_batch(shuffle=True):\n",
    "                step, batch_loss = model.run_step(sess, True, batch)\n",
    "                loss.append(batch_loss)\n",
    "                if step % FLAGS.steps_check == 0:\n",
    "                    iteration = step // steps_per_epoch + 1\n",
    "                    logger.info(\"iteration:{} step:{}/{}, \"\n",
    "                                \"NER loss:{:>9.6f}\".format(\n",
    "                        iteration, step%steps_per_epoch, steps_per_epoch, np.mean(loss)))\n",
    "                    loss = []\n",
    "\n",
    "            best = evaluate(sess, model, \"dev\", dev_manager, id_to_tag, logger)\n",
    "            if best:\n",
    "                save_model(sess, model, FLAGS.ckpt_path, logger)\n",
    "            # evaluate(sess, model, \"test\", test_manager, id_to_tag, logger)\n",
    "\n",
    "def main(_):\n",
    "\n",
    "    if FLAGS.train:\n",
    "        if FLAGS.clean:\n",
    "            clean(FLAGS)\n",
    "        train()\n",
    "    else:\n",
    "        evaluate_line()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.app.run(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\leo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:417: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n",
      "WARNING:tensorflow:From C:\\Users\\leo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:432: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-18 14:56:57,174 - train.log - INFO - Reading model parameters from ckpt\\ner.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ckpt\\ner.ckpt\n",
      "哮喘古代文献也称“鼻息”、“肩息”、“上气”等。\n",
      "[('哮喘', 'DISEASE'), ('鼻息', 'DISEASE'), ('肩息', 'DISEASE')]\n",
      "喘病是指由于外感或内伤，导致肺失宣降，肺气上逆或气无所主，肾失摄纳，以致呼吸困难，甚则张口抬肩，鼻翼煽动，不能平卧等为主要临床特征的一种病证。 \n",
      "[('喘病', 'SYMPTOM'), ('肺', 'BODY'), ('肺', 'BODY'), ('肾', 'BODY'), ('呼吸困难', 'SYMPTOM'), ('口', 'BODY'), ('肩', 'BODY'), ('鼻翼', 'BODY')]\n"
     ]
    }
   ],
   "source": [
    "def evaluate_line():\n",
    "    config = load_config(FLAGS.config_file)\n",
    "    logger = get_logger(FLAGS.log_file)\n",
    "    # limit GPU memory\n",
    "    tf_config = tf.ConfigProto()\n",
    "    tf_config.gpu_options.allow_growth = True\n",
    "    with open(FLAGS.map_file, \"rb\") as f:\n",
    "        char_to_id, id_to_char, tag_to_id, id_to_tag = pickle.load(f)\n",
    "    with tf.Session(config=tf_config) as sess:\n",
    "        model = create_model(sess, Model, FLAGS.ckpt_path, load_word2vec, config, id_to_char, logger)\n",
    "        # while True:\n",
    "            # try:\n",
    "            #     line = input(\"请输入测试句子:\")\n",
    "            #     result = model.evaluate_line(sess, input_from_line(line, char_to_id), id_to_tag)\n",
    "            #     print(result)\n",
    "            # except Exception as e:\n",
    "            #     logger.info(e)\n",
    "\n",
    "                # line = input(\"请输入测试句子:\")\n",
    "        line = \"哮喘古代文献也称“鼻息”、“肩息”、“上气”等。\"\n",
    "        result = model.evaluate_line(sess, input_from_line(line, char_to_id), id_to_tag)\n",
    "        print(line)\n",
    "        print([(x[\"word\"],x[\"type\"]) for x in result[\"entities\"]])\n",
    "\n",
    "        line = \"喘病是指由于外感或内伤，导致肺失宣降，肺气上逆或气无所主，肾失摄纳，以致呼吸困难，甚则张口抬肩，鼻翼煽动，不能平卧等为主要临床特征的一种病证。 \"\n",
    "        result = model.evaluate_line(sess, input_from_line(line, char_to_id), id_to_tag)\n",
    "        print(line)\n",
    "        print([(x[\"word\"],x[\"type\"]) for x in result[\"entities\"]])\n",
    "\n",
    "evaluate_line()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
